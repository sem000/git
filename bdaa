pract 1a k-means
library (plyr)
library(ggplot2) 
library(cluster) 
library(lattice) 
library(graphics)
library(grid) 
library(gridExtra)
grade_input = as.data.frame(read.csv("D:/Clustering_master_grades_km_input.csv.csv"))
kmdata_orig = as.matrix(grade_input [, c ("Student" , "English" , "Math" , "Science")])
kmdata <-kmdata_orig[,2:4] 
kmdata[1:10,] 
wss <-numeric(15 )
for(k in 1:15) wss[k] <- sum(kmeans(kmdata,centers=k,nstart=25)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters" , ylab="within Sum of squares")
km=kmeans(kmdata,3,nstart=25)
km
c( wss[3] , sum(km$withinss))
df=as.data.frame(kmdata_orig[,2:4])
df$cluster=factor(km$cluster)
centers=as.data.frame(km$centers)
g1=ggplot(data=df, aes(x=English, y=Math, color=cluster )) +
  geom_point() + theme(legend.position="right") +
  geom_point(data=centers,aes(x=English,y=Math, color=as.factor(c(1,2,3))),size=10, alpha=.3,
             show.legend =FALSE)
g2=ggplot(data=df, aes(x=English, y=Science, color=cluster )) +
  geom_point () +geom_point(data=centers,aes(x=English,y=Science,
                                             color=as.factor(c(1,2,3))),size=10, alpha=.3, show.legend=FALSE)
g3 = ggplot(data=df, aes(x=Math, y=Science, color=cluster )) +
  geom_point () + geom_point(data=centers,aes(x=Math,y=Science,
                                              color=as.factor(c(1,2,3))),size=10, alpha=.3, show.legend=FALSE)
tmp=ggplot_gtable(ggplot_build(g1))
grid.arrange(arrangeGrob(g1 + theme(legend.position="none"),g2 + theme(legend.position="none"),g3 + theme(legend.position="none"),top ="High School Student Cluster Analysis" ,ncol=1))

pract 1 b apriori
install.packages("arules")
install.packages("arulesViz")
install.packages("RColorBrewer")
library(arules)
library(arulesViz)
library(RColorBrewer)
data(Groceries)
Groceriessummary(Groceries)
class(Groceries)
rules = apriori(Groceries, parameter = list(supp = 0.02, conf = 0.2))
summary (rules)
inspect(rules[1:10])
arules::itemFrequencyPlot(Groceries, topN = 20, 
                          col = brewer.pal(8, 'Pastel2'),
                          main = 'Relative Item Frequency Plot',
                          type = "relative",
                          ylab = "Item Frequency (Relative)")
itemsets = apriori(Groceries, parameter = list(minlen=2, maxlen=2,support=0.02, target="frequent itemsets"))

summary(itemsets)
inspect(itemsets[1:10])
itemsets_3 = apriori(Groceries, parameter = list(minlen=3, maxlen=3,support=0.02, target="frequent itemsets"))
summary(itemsets_3)
inspect(itemsets_3)

pract 2 a Logistic Regression cgpa
college <- read.csv("D:/csquared_udacity-dlnd_master_nn_binary.csv")
head(college)
nrow(college)
install.packages("caTools")
library(caTools)
split <- sample.split(college, SplitRatio = 0.75)
split
training_reg <- subset(college, split == "TRUE")
test_reg <- subset(college, split == "FALSE")
fit_logistic_model <- glm(admit ~ .,
                          data = training_reg,
                          family = "binomial")
predict_reg <- predict(fit_logistic_model,
                       test_reg, type = "response")
predict_reg
cdplot(as.factor(admit)~gpa, data=college)
cdplot(as.factor(admit)~gre, data=college)
cdplot(as.factor(admit)~rank, data=college)
predict_reg <- ifelse(predict_reg > 0.5, 1, 0)
predict_reg
table(test_reg$admit, predict_reg)

pract 2 a Logistic Regression ISLR
install.packages("ISLR")
library(ISLR)
data <- ISLR::Default
print (head(ISLR::Default))
summary(data)
nrow(data)
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
print (sample)
train <- data[sample, ]
test <- data[!sample, ]
nrow(train)
nrow(test)
model <- glm(default~student+balance+income, family="binomial", data=train)
summary(model)
install.packages("InformationValue")
library(InformationValue)
predicted <- predict(model, test, type="response")
confusionMatrix(test$default, predicted)

pract 2 b Multiple Regression
college <- read.csv("D:/csquared_udacity-dlnd_master_nn_binary.csv")
head(college)
nrow(college)
install.packages("caTools") 
library(caTools)
split <- sample.split(college, SplitRatio = 0.75)
split
training_reg <- subset(college, split == "TRUE")
test_reg <- subset(college, split == "FALSE")
fit_MRegression_model <- lm(formula = admit ~ gre+gpa+rank,
                            data = training_reg)
predict_reg <- predict(fit_MRegression_model,
                       newdate = test_reg)
predict_reg
cdplot(as.factor(admit)~ gpa, data=college)
cdplot(as.factor(admit)~ gre, data=college)
cdplot(as.factor(admit)~ rank, data=college)

pract 2 c Linear Regression cgpa
college <- read.csv("D:/csquared_udacity-dlnd_master_nn_binary.csv")
head(college)
nrow(college)
install.packages("caTools")
library(caTools)
split <- sample.split(college,SplitRatio = 0.75)
split
training_reg <- subset(college, split=="TRUE")
test_reg <- subset(college, split=="FALSE")
fit_MRegrssor_model <- lm(formula = admit ~ gre, data = training_reg)
predict_reg <- predict(fit_MRegrssor_model,newdata=test_reg)
predict_reg
cdplot(as.factor(admit)~gre, data = college)

pract 2c linear employye
years_of_exp = c(7,5,1,3)
salary_in_lakhs = c(21,13,6,8)
#employee.data = data.frame(satisfaction_score, years_of_exp, salary_in_lakhs)
employee.data = data.frame(years_of_exp, salary_in_lakhs)
employee.data
# Estimation of the salary of an employee, based on his year of experience and satisfaction score in his company.
model <- lm(salary_in_lakhs ~ years_of_exp, data = employee.data)
summary(model)
# The formula of Regression becomes
# Y = 2 + 2.5*year_of_Exp
# Visualization of Regression
plot(salary_in_lakhs ~ years_of_exp, data = employee.data)
abline(model)

pract 3 a decision tress
dataset = read.csv("D:/Social_Network_Ads.csv")
dataset = dataset[3:5]
print(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
print(dataset$Purchased)
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
install.packages('rpart')
library(rpart)
classifier = rpart(formula = Purchased ~ .,data = training_set)
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
cm = table(test_set[, 3], y_pred)
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
     main = 'Decision Tree Classification (Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'Decision Tree Classification (Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
plot(classifier)
text(classifier)

pract 3 b svm
dataset = read.csv("D:/Social_Network_Ads.csv")
dataset = dataset[3:5]
print(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
print(dataset$Purchased)
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
print(training_set[-3])
print(test_set[-3])
install.packages('e1071')
library(e1071)
classifier = svm(formula = Purchased ~ .,data = training_set,type='C-classification',kernel='linear')
print(classifier)
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')
cm = table(test_set[, 3], y_pred)
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3],
     main = 'SVM(Training set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class')
plot(set[, -3], main = 'SVM(Test set)',
     xlab = 'Age', ylab = 'Estimated Salary',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
plot(classifier)
text(classifier)

pract 4 a naiveBayes
dataset = read.csv("D:/Social_Network_Ads.csv") 
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
install.packages('caTools')
library(caTools) 
set.seed(123)
split = sample.split(dataset $Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE) 
training_set[-3]= scale(training_set[-3]) 
test_set[-3]= scale(test_set[-3])
install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],y= training_set$Purchased)
y_pred = predict(classifier, newdata = test_set[-3])
cm= table(test_set[, 3], y_pred)
print(cm)

pract 4 b txt anyalsis
dataset_original = read.delim("D:/Restaurant_Reviews.tsv", quote = '', stringsAsFactors = FALSE)
install.packages('tm')
install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
print(dataset$Liked)
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
                          y = training_set$Liked,
                          ntree = 10)
y_pred = predict(classifier, newdata = test_set[-692])
cm = table(test_set[, 692], y_pred)
print(cm)

pract 5 Comparative study
library(rpart) 
library(rpart.plot) 
library(gmodels) 
library(e1071) 
data(iris) 
summary(iris) 
temp=as.data.frame(scale(iris[,1:4])) 
temp$Species=iris$Species 
summary(temp) 
library(caTools) 
set.seed(123) 
split=sample.split(temp$Species,SplitRatio=0.75) 
train=subset(temp,split==TRUE) 
test=subset(temp,split==FALSE) 
nrow(train) 
nrow(test) 
dt_classifier=rpart(formula=Species~.,data=train) 
dt_y_pred=predict(dt_classifier,newdata=test,type='class') 
print(dt_y_pred) 
cm=table(test$Species,dt_y_pred) 
print(cm) 
DTaccu=((12+9+11)/nrow(test))*100 
DTaccu 
library(class) 
c1=train$Species 
set.seed(1234) 
knn_y_pred=knn(train[,1:4],test[,1:4],c1,k=5) 
cm=table(test$Species,knn_y_pred) 
print(cm) 
KNNaccu=((12+11+11)/nrow(test))*100 
KNNaccu 
svmclassifier=svm(Species~.,data=train) 
svm_y_pred=predict(svmclassifier,newdata=test) 
cm=table(test$Species,svm_y_pred) 
print(cm) 
SVMaccu=((12+11+11)/nrow(test))*100 
SVMaccu 
which(dt_y_pred!=knn_y_pred) 
which(dt_y_pred!=svm_y_pred) 
which(svm_y_pred!=knn_y_pred) 
models=data.frame(Technique=c("Decision Tree","Knn","SVM"),Accuracy_Percentage=c(88.88889,94.44444,94.44444)) 
models 

pract 6 install hdoop
Steps to Install Hadoop
1.	Install Java JDK 1.8
2.	Download Hadoop and extract and place under C drive
3.	Set Path in Environment Variables
4.	Config files under Hadoop directory
5.	Create folder datanode and namenode under data directory
6.	Edit HDFS and YARN files
7.	Set Java Home environment in Hadoop environment
8.	Setup Complete. Test by executing start-all.cmd
